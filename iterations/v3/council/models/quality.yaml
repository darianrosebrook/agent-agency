# Quality Evaluator Model Specification
# Assesses output correctness, completeness, maintainability

model:
  base: "gemma2:3b"
  name: "gemma2:3b-quality-judge"
  description: "Specialized judge for output quality and acceptance criteria evaluation"

# Training Configuration
training:
  method: "LoRA"
  rank: 16
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  
  # Training data sources
  datasets:
    - name: "acceptance-criteria"
      source: "training/caws-dataset/acceptance-examples.jsonl"
      weight: 1.0
    - name: "quality-patterns"
      source: "training/caws-dataset/quality-examples.jsonl"
      weight: 0.9
    - name: "maintainability-examples"
      source: "training/caws-dataset/maintainability-examples.jsonl"
      weight: 0.8

# Performance Targets
performance:
  inference_latency_ms: 200  # Target <200ms balanced CPU/GPU
  memory_usage_gb: 3.0
  throughput_tokens_per_sec: 40

# Optimization
optimization:
  target: "CPU"  # Balanced CPU/GPU execution
  quantization: "INT8"
  batch_size: 1

# Prompt Template
prompt_template: |
  You are the Quality Evaluator for the Agent Agency Council. Your role is to assess 
  whether agent outputs meet acceptance criteria and quality standards.

  ## Your Responsibilities:
  1. **Acceptance Criteria Validation**: Verify all specified requirements are met
  2. **Completeness Assessment**: Ensure outputs are complete and comprehensive
  3. **Correctness Evaluation**: Check for logical errors and factual accuracy
  4. **Maintainability Review**: Assess code/documentation quality for long-term maintenance
  5. **User Experience**: Evaluate usability and user-facing quality aspects

  ## Evaluation Criteria:

  ### Acceptance Criteria
  - All specified requirements addressed
  - Edge cases handled appropriately
  - Error conditions properly managed
  - Performance requirements met

  ### Completeness
  - All components implemented as specified
  - Documentation and comments included
  - Tests provided where appropriate
  - Integration points properly handled

  ### Correctness
  - Logic errors and bugs identified
  - Data validation and error handling
  - Business rule compliance
  - Algorithm and implementation accuracy

  ### Maintainability
  - Code organization and structure
  - Documentation quality and clarity
  - Testing coverage and quality
  - Future extensibility considerations

  ## Response Format:
  You must respond with a JSON verdict containing:
  ```json
  {
    "verdict": "pass|fail|uncertain",
    "confidence": 0.0-1.0,
    "reasoning": "Quality assessment summary",
    "evidence": [
      {
        "source": "TestResults|Documentation|CodeAnalysis",
        "content": "Specific quality evidence",
        "relevance": 0.0-1.0
      }
    ],
    "violations": [
      {
        "rule": "Quality standard or criterion",
        "severity": "critical|major|minor|warning",
        "description": "Quality issue description",
        "location": "Component or section",
        "suggestion": "How to improve quality"
      }
    ],
    "quality_metrics": {
      "completeness_score": 0.85,
      "correctness_score": 0.92,
      "maintainability_score": 0.78,
      "overall_quality": 0.85
    }
  }
  ```

  ## Output to Evaluate:
  **Task ID**: {{task_id}}
  **Acceptance Criteria**: {{acceptance_criteria}}
  **Worker Output**: {{worker_output}}
  **Context**: {{context}}

  Evaluate the quality and completeness of this output against the specified criteria.

# Training Dataset Examples
training_examples:
  acceptance_criteria:
    - input: |
        Criteria: "Implement user login with email/password validation"
        Output: Login form with email field only, no password validation
      output: |
        {
          "verdict": "fail",
          "confidence": 0.9,
          "reasoning": "Missing password field and validation logic",
          "violations": [
            {
              "rule": "Complete Implementation",
              "severity": "critical",
              "description": "Password field and validation missing",
              "location": "Login form component",
              "suggestion": "Add password field and implement validation logic"
            }
          ],
          "quality_metrics": {
            "completeness_score": 0.4,
            "correctness_score": 0.3,
            "maintainability_score": 0.7,
            "overall_quality": 0.45
          }
        }
  
  quality_standards:
    - input: |
        Output: Well-documented code with comprehensive tests and error handling
      output: |
        {
          "verdict": "pass",
          "confidence": 0.95,
          "reasoning": "High-quality implementation with excellent documentation and testing",
          "evidence": [
            {
              "source": "Documentation",
              "content": "Comprehensive code documentation and comments",
              "relevance": 0.9
            }
          ],
          "quality_metrics": {
            "completeness_score": 0.95,
            "correctness_score": 0.92,
            "maintainability_score": 0.98,
            "overall_quality": 0.95
          }
        }

# Quality Assurance
quality_assurance:
  test_cases:
    - name: "acceptance_criteria_validation"
      description: "Verify accurate acceptance criteria evaluation"
    - name: "completeness_assessment"
      description: "Verify completeness scoring accuracy"
    - name: "quality_metric_calculation"
      description: "Verify quality metric calculations"

  benchmarks:
    - name: "acceptance_criteria_accuracy"
      target: 0.90  # 90% accuracy on acceptance criteria evaluation
    - name: "quality_assessment_consistency"
      target: 0.88  # 88% consistency in quality assessments
    - name: "false_negative_rate"
      target: 0.05  # <5% false negative rate

# Deployment Configuration
deployment:
  platform: "Apple Silicon"
  runtime: "Core ML"
  model_format: "mlpackage"
  optimization_level: "maximum"
  
  # Resource allocation
  resources:
    cpu_cores: 4
    memory_limit_mb: 3072
    thermal_threshold_c: 85
    
  # Monitoring
  monitoring:
    - metric: "inference_latency"
      threshold_ms: 200
      alert: true
    - metric: "quality_assessment_accuracy"
      threshold: 0.90
      alert: true
