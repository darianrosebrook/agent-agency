# Alertmanager configuration for Multimodal RAG System
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@multimodal-rag.com'
  smtp_auth_username: '${SMTP_USERNAME}'
  smtp_auth_password: '${SMTP_PASSWORD}'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'email-alerts'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true
    - match:
        severity: warning
      receiver: 'warning-alerts'
    - match:
        alertname: 'SLABreach|SLABreachResponseTime'
      receiver: 'slo-alerts'

receivers:
  - name: 'email-alerts'
    email_configs:
      - to: '${ALERT_EMAIL}'
        subject: '{{ template "email.subject" . }}'
        body: '{{ template "email.body" . }}'
        send_resolved: true

  - name: 'critical-alerts'
    email_configs:
      - to: '${CRITICAL_ALERT_EMAIL}'
        subject: ' CRITICAL: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
           **CRITICAL ALERT**

          **Alert:** {{ .Annotations.summary }}
          **Description:** {{ .Annotations.description }}
          **Severity:** {{ .Labels.severity }}
          **Time:** {{ .StartsAt.Format "2006-01-02 15:04:05" }}

          **Labels:**
          {{ range $key, $value := .Labels }}{{ $key }}: {{ $value }}
          {{ end }}

          **Annotations:**
          {{ range $key, $value := .Annotations }}{{ $key }}: {{ $value }}
          {{ end }}
          {{ end }}
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#critical-alerts'
        title: ' CRITICAL ALERT: {{ .GroupLabels.alertname }}'
        text: '{{ .CommonAnnotations.description }}'
        send_resolved: true

  - name: 'warning-alerts'
    email_configs:
      - to: '${WARNING_ALERT_EMAIL}'
        subject: '⚠️ WARNING: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          ⚠️ **WARNING ALERT**

          **Alert:** {{ .Annotations.summary }}
          **Description:** {{ .Annotations.description }}
          **Severity:** {{ .Labels.severity }}
          **Time:** {{ .StartsAt.Format "2006-01-02 15:04:05" }}

          **Labels:**
          {{ range $key, $value := .Labels }}{{ $key }}: {{ $value }}
          {{ end }}

          **Annotations:**
          {{ range $key, $value := .Annotations }}{{ $key }}: {{ $value }}
          {{ end }}
          {{ end }}
        send_resolved: true

  - name: 'slo-alerts'
    email_configs:
      - to: '${SLO_ALERT_EMAIL}'
        subject: ' SLO ALERT: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
           **SLO ALERT**

          **Alert:** {{ .Annotations.summary }}
          **Description:** {{ .Annotations.description }}
          **SLO Impact:** This alert indicates potential service level degradation
          **Time:** {{ .StartsAt.Format "2006-01-02 15:04:05" }}

          **Labels:**
          {{ range $key, $value := .Labels }}{{ $key }}: {{ $value }}
          {{ end }}

          **Annotations:**
          {{ range $key, $value := .Annotations }}{{ $key }}: {{ $value }}
          {{ end }}
          {{ end }}
        send_resolved: true
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#slo-alerts'
        title: ' SLO ALERT: {{ .GroupLabels.alertname }}'
        text: 'SLO breach detected: {{ .CommonAnnotations.description }}'
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname']

  - source_match:
      alertname: 'SLABreach'
    target_match_re:
      alertname: 'HighErrorRate|HighResponseTime|DatabaseConnectionIssues|RedisConnectionIssues'
    equal: ['instance']
