id: AGENT-0002
title: "Agent Agency V2: Agentic RL & Extended Thinking"
risk_tier: 2
mode: feature
change_budget:
  max_files: 35
  max_loc: 1500
blast_radius:
  modules: ["src/rl/", "src/thinking/", "src/mcp/evaluation/", "src/mcp/tools/"]
  data_migration: false
operational_rollback_slo: "10m"
threats:
  - "RL training instability during multi-turn interactions"
  - "Thinking budget exhaustion in complex tasks"
  - "Reward hacking in minimal-diff enforcement"
scope:
  in: ["iterations/v1/"]
  out: ["iterations/poc/", "node_modules/", "dist/"]
invariants:
  - "System maintains backward compatibility with V1 agent orchestration"
  - "RL training data never includes sensitive tenant information"
  - "Thinking budgets prevent infinite loops in evaluation cycles"
  - "Minimal-diff enforcement doesn't break existing agent workflows"
acceptance:
  - id: "A1"
    given: "Agent with extended thinking budget"
    when: "Task complexity increases"
    then: "System automatically allocates more thinking tokens"
  - id: "A2"
    given: "Multi-turn tool interaction"
    when: "Tool use provides relevant information"
    then: "Turn-level rewards reinforce correct tool choice"
  - id: "A3"
    given: "Code generation task"
    when: "Agent produces solution with unnecessary scaffolding"
    then: "Minimal-diff evaluator penalizes reward-hacking behavior"
  - id: "A4"
    given: "Tool learning scenario"
    when: "Small model ignores available tools"
    then: "SFT warmup and intermediate rewards improve tool adoption"
  - id: "A5"
    given: "Agent with comprehensive rubric"
    when: "Task surface changes from code-editing to research"
    then: "Reward weights adapt automatically for optimal performance"
  - id: "A6"
    given: "RL training episode with failure mode detection"
    when: "Dummy tool use or reward hacking detected"
    then: "Specific mitigation activates and training adjusts curriculum"
  - id: "A7"
    given: "Formal RL environment interface"
    when: "Agent takes action in multi-turn scenario"
    then: "Environment provides structured observation and reward computation"
non_functional:
  a11y: ["keyboard-navigation", "screen-reader-labels"]
  perf:
    api_p95_ms: 500  # Increased for RL inference
    thinking_budget_max_ms: 10000
    tool_call_p95_ms: 200
  security: ["tenant-isolation", "rl-data-privacy", "tool-permission-scopes"]
contracts:
  - type: "typescript"
    path: "src/types/agentic-rl.ts"
observability:
  logs:
    - "thinking_budget_utilization"
    - "tool_choice_rewards"
    - "minimal_diff_penalties"
  metrics:
    - "thinking_token_efficiency"
    - "tool_adoption_rate"
    - "reward_hacking_incidents"
  traces:
    - "multi_turn_conversation_flows"
    - "rl_training_iterations"
    - "evaluation_loop_performance"
migrations: []
rollback:
  - "Disable RL features via feature flags"
  - "Revert to V1 thinking budget defaults"
  - "Remove minimal-diff penalties from evaluation"
ai_assessment:
  confidence_level: 0.85
  uncertainty_areas:
    - "Long-term RL training stability in production"
    - "Optimal thinking budget allocation curves"
  complexity_factors:
    - "Multi-tenant RL data isolation"
    - "Turn-level credit assignment"
    - "Model-based reward computation"
  risk_factors:
    - "RL training could introduce unpredictable agent behavior"
    - "Thinking budget optimization might reduce task throughput"
