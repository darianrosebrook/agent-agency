"""
DSPy Signature for Rubric Optimization

Systematic optimization of reward computation prompts for agent behavior evaluation.

@author @darianrosebrook
"""

import dspy
from typing import Any


class RubricOptimization(dspy.Signature):
    """
    Optimize reward computation for agent behavior evaluation.

    This signature defines the interface for DSPy to systematically optimize
    how we evaluate agent outputs and compute reward scores.
    """

    task_context: str = dspy.InputField(
        desc="Description of the agent task being evaluated"
    )
    agent_output: str = dspy.InputField(
        desc="The actual output generated by the agent"
    )
    evaluation_criteria: str = dspy.InputField(
        desc="Specific criteria to evaluate against (e.g., 'JSON validity', 'Schema compliance')"
    )

    reward_score: float = dspy.OutputField(
        desc="Optimized reward score from 0.0 to 1.0, where 1.0 is perfect"
    )
    reasoning: str = dspy.OutputField(
        desc="Detailed explanation of how the score was determined, referencing specific criteria"
    )
    improvement_suggestions: str = dspy.OutputField(
        desc="Specific, actionable suggestions for how the agent could improve its output"
    )


class RubricOptimizer(dspy.Module):
    """
    DSPy module for systematic rubric optimization.

    This module uses DSPy's automatic prompt optimization to improve
    the quality and consistency of reward computation over time.
    """

    def __init__(self):
        """Initialize the rubric optimizer module."""
        super().__init__()
        self.optimizer = dspy.Predict(RubricOptimization)

    def forward(
        self,
        task_context: str,
        agent_output: str,
        evaluation_criteria: str
    ) -> dspy.Prediction:
        """
        Optimize rubric evaluation for given inputs.

        Args:
            task_context: Description of the agent task
            agent_output: Agent's generated output
            evaluation_criteria: Criteria to evaluate

        Returns:
            Prediction containing reward_score, reasoning, and improvement_suggestions
        """
        return self.optimizer(
            task_context=task_context,
            agent_output=agent_output,
            evaluation_criteria=evaluation_criteria
        )

    def compile(
        self,
        trainset: list[dspy.Example],
        optimizer: Any = None,
        metric: Any = None
    ):
        """
        Compile and optimize the module using evaluation data.

        Args:
            trainset: Training examples with ground truth evaluations
            optimizer: DSPy optimizer to use (e.g., MIPROv2, SIMBA)
            metric: Evaluation metric function

        Returns:
            Compiled and optimized module
        """
        if optimizer is None:
            optimizer = dspy.MIPROv2(
                metric=metric,
                num_candidates=10,
                init_temperature=1.0
            )

        return optimizer.compile(
            self,
            trainset=trainset,
            num_trials=100
        )


def create_rubric_example(
    task_context: str,
    agent_output: str,
    evaluation_criteria: str,
    expected_score: float,
    expected_reasoning: str,
    expected_suggestions: str
) -> dspy.Example:
    """
    Create a training example for rubric optimization.

    Args:
        task_context: Task description
        agent_output: Agent's output
        evaluation_criteria: Evaluation criteria
        expected_score: Ground truth score
        expected_reasoning: Ground truth reasoning
        expected_suggestions: Ground truth suggestions

    Returns:
        DSPy Example for training
    """
    return dspy.Example(
        task_context=task_context,
        agent_output=agent_output,
        evaluation_criteria=evaluation_criteria,
        reward_score=expected_score,
        reasoning=expected_reasoning,
        improvement_suggestions=expected_suggestions
    ).with_inputs(
        "task_context",
        "agent_output",
        "evaluation_criteria"
    )
