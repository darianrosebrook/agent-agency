id: RL-003
title: "ModelBasedJudge - LLM-as-Judge for Subjective Evaluation"
version: "1.0.0"
mode: feature
risk_tier: 2
status: spec_complete

executive_summary:
  purpose: |
    The ModelBasedJudge implements LLM-as-judge evaluation for subjective quality assessment in RL training.
    It provides confidence-scored judgments across multiple criteria (faithfulness, relevance, minimality, safety)
    to complement rule-based evaluation metrics.
  scope:
    - LLM integration for judgment generation
    - Confidence scoring (0-1) for each assessment
    - Multi-criteria evaluation (4 dimensions)
    - Prompt engineering for consistent, reliable judgments
  success_criteria:
    - Generate confidence-scored assessments for all criteria
    - Consistent judgments for identical inputs (deterministic prompting)
    - Judgments complete within 500ms P95
    - Assessment accuracy ≥ 85% compared to human evaluation

change_budget:
  max_files: 15
  max_loc: 650

blast_radius:
  modules: ["src/evaluation", "src/rl", "src/ai"]
  data_migration: false
  breaking_changes: false
  external_impact: medium

operational_rollback_slo: "5m"

threats:
  - "LLM API failures causing evaluation disruption"
  - "Inconsistent judgments due to temperature settings"
  - "Prompt injection attacks via evaluated content"
  - "Cost escalation from excessive LLM calls"

scope:
  in:
    - "iterations/v2/src/evaluation/"
    - "iterations/v2/src/types/judge.ts"
    - "iterations/v2/tests/unit/evaluation/"
  out:
    - "node_modules/"
    - "dist/"
    - "iterations/poc/"

invariants:
  - "Confidence scores always between 0.0 and 1.0 inclusive"
  - "All four evaluation criteria assessed for every judgment"
  - "Judgment generation uses temperature=0 for determinism"
  - "Fallback to rule-based evaluation if LLM unavailable"

acceptance:
  - id: "RL-003-A1"
    given: "Agent output requires faithfulness assessment"
    when: "ModelBasedJudge evaluates factual accuracy"
    then: "Faithfulness score provided with confidence ≥ 0.7"

  - id: "RL-003-A2"
    given: "Agent output requires relevance assessment"
    when: "Judge evaluates task alignment"
    then: "Relevance score provided with confidence ≥ 0.7"

  - id: "RL-003-A3"
    given: "Agent output requires minimality assessment"
    when: "Judge evaluates solution elegance"
    then: "Minimality score provided with confidence ≥ 0.7"

  - id: "RL-003-A4"
    given: "Agent output requires safety assessment"
    when: "Judge evaluates potential harms"
    then: "Safety score provided with confidence ≥ 0.8"

  - id: "RL-003-A5"
    given: "Identical inputs provided twice"
    when: "Judge generates assessments"
    then: "Judgments are consistent (deterministic prompting with temp=0)"

  - id: "RL-003-A6"
    given: "High-volume evaluation workload (100+ judgments)"
    when: "Performance measured under load"
    then: "All judgments complete within 500ms P95 budget"

non_functional:
  performance:
    judgment_generation_p95_ms: 500
    confidence_calculation_p95_ms: 50
    prompt_construction_p95_ms: 20
    max_concurrent_judgments: 50
    memory_usage_mb: 50

  reliability:
    availability_percent: 99.5
    error_rate_percent: 1.0
    graceful_degradation: true
    fallback_enabled: true

  scalability:
    max_concurrent_evaluations: 50
    judgment_throughput_per_sec: 20
    horizontal_scaling: true

  security:
    input_validation: "strict"
    prompt_injection_prevention: "enabled"
    content_sanitization: "required"
    rate_limiting: "per-tenant"

  usability:
    api_design: "async-pipeline"
    error_messages: "llm-error-context"
    documentation: "prompt-templates-documented"

contracts:
  - type: "typescript"
    path: "src/types/judge.ts"
    version: "1.0.0"
    description: "Judge interfaces and evaluation types"

  - type: "typescript"
    path: "src/evaluation/ModelBasedJudge.ts"
    version: "1.0.0"
    description: "Main judge implementation"

  - type: "typescript"
    path: "src/evaluation/ConfidenceScorer.ts"
    version: "1.0.0"
    description: "Confidence calculation logic"

observability:
  metrics:
    - "judgment_latency_p95"
    - "confidence_score_distribution"
    - "llm_api_success_rate"
    - "evaluation_criteria_scores"

  logs:
    - level: "info"
      events: ["judgment_generated", "confidence_calculated", "criteria_assessed"]
    - level: "warn"
      events: ["low_confidence_judgment", "llm_api_slow", "fallback_triggered"]
    - level: "error"
      events: ["llm_api_failed", "judgment_timeout", "invalid_response"]

  traces:
    - "judgment_generation_span"
    - "llm_api_call_span"
    - "confidence_scoring_span"

migrations: []

rollback:
  slo: "5m"
  strategy: "feature-flag-with-fallback"
  impact: "low"
  monitoring: "judge_disabled_fallback_active"

ai_assessment:
  reasoning: "LLM-as-judge requires careful prompt engineering and API integration. The challenge is ensuring consistency and handling API failures gracefully. AI can implement the integration logic but human oversight critical for prompt design and fallback strategies."

  risks:
    - "LLM API rate limits and costs"
    - "Inconsistent judgments across model versions"
    - "Prompt injection vulnerabilities"

  opportunities:
    - "Well-established LLM-as-judge literature"
    - "Clear evaluation criteria definitions"
    - "Existing LLM client libraries"

  recommendations:
    - "Use temperature=0 for deterministic results"
    - "Implement comprehensive fallback to rule-based evaluation"
    - "Cache identical judgments to reduce API calls"
    - "Include extensive prompt testing with adversarial inputs"
    - "Monitor LLM API costs and implement budget controls"

